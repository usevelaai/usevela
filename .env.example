# vela Configuration
# Copy this file to .env and modify as needed

# ===================
# Core Settings
# ===================
# Set to true for self-hosted (disables billing/usage limits)
SELF_HOSTED=true
NEXT_PUBLIC_SELF_HOSTED=true

# Generate a random secret (minimum 32 characters): openssl rand -base64 32
BETTER_AUTH_SECRET=change-me-to-a-secure-secret-at-least-32-chars

# ===================
# Database
# ===================
# For local development (no Docker)
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/vela
# For Docker Compose
# DATABASE_URL=postgresql://postgres:postgres@postgres:5432/vela

# ===================
# LLM Provider
# ===================
# Option 1: Use Ollama locally (default for self-hosted)
OPENAI_API_BASE=http://localhost:11434/v1
OPENAI_API_KEY=ollama
OPENAI_MODEL=llama3.2

# Option 2: Use OpenAI directly
# OPENAI_API_BASE=https://api.openai.com/v1
# OPENAI_API_KEY=sk-your-key
# OPENAI_MODEL=gpt-4o-mini

# Option 3: Use Anthropic (cloud default)
# ANTHROPIC_API_KEY=sk-ant-your-key

# ===================
# Embeddings
# ===================
# For self-hosted, you can use Ollama embeddings or skip this
# VOYAGE_API_KEY=

# ===================
# URLs
# ===================
DASHBOARD_URL=http://localhost:3000
NEXT_PUBLIC_API_URL=http://localhost:3001
NEXT_PUBLIC_WIDGET_URL=http://localhost:3002
VITE_API_URL=http://localhost:3001

# ===================
# Emails
# ===================
RESEND_API_KEY=re_xxxxx
EMAIL_FROM=Vela <noreply@usevela.ai>

# ===================
# OAuth (Optional)
# ===================
# GOOGLE_CLIENT_ID=
# GOOGLE_CLIENT_SECRET=
# GITHUB_CLIENT_ID=
# GITHUB_CLIENT_SECRET=
